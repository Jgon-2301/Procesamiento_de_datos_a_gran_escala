{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64783155-5357-450d-a3b7-b1c8d083361b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Procesamiento de Datos a Gran Escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae966711-f914-497f-8e03-d28bf1488bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pontificia Universidad Javeriana\n",
    "\n",
    "*Autores:*\n",
    "- Juan Felipe González Quintero  \n",
    "\n",
    "*Fecha de Inicio:* 27 - 10 - 2025  \n",
    "*Fecha de Finalización:* 27 - 10 - 2025  \n",
    "\n",
    "---\n",
    "\n",
    "### Temática\n",
    "*Optimización de modelos de clasificación mediante ajuste de hiperparámetros en PySpark.*\n",
    "\n",
    "---\n",
    "\n",
    "## *Problemática*\n",
    "En los procesos de modelado predictivo, la elección de los hiperparámetros adecuados puede marcar la diferencia entre un modelo preciso y uno con bajo rendimiento.  \n",
    "Este laboratorio busca explorar cómo las variaciones en los parámetros de regularización y la formulación de las variables influyen en el desempeño de un modelo de regresión logística dentro del entorno distribuido de PySpark, evaluando la eficacia de cada configuración a partir de métricas cuantitativas.\n",
    "\n",
    "---\n",
    "\n",
    "## *Objetivo*\n",
    "Analizar y comparar el rendimiento de diferentes combinaciones de hiperparámetros en un modelo de regresión logística aplicado a un conjunto de datos multivariable, utilizando métricas de desempeño basandose en un flujo básico de machine learning\n",
    "El propósito es determinar qué ajustes optimizan la capacidad predictiva del modelo, maximizando la discriminación entre clases y reduciendo los errores de clasificación.\n",
    "\n",
    "---\n",
    "\n",
    "## *Metodología*\n",
    "\n",
    "1. **Carga y preparación de datos**  \n",
    "   Se importó un conjunto de datos en formato JSON desde los recursos de Databricks, aplicando transformaciones mediante *RFormula* para generar variables dependientes e independientes.\n",
    "\n",
    "2. **Construcción del pipeline de modelado**  \n",
    "   Se configuró un *pipeline* con las etapas de transformación (*RFormula*) y clasificación (*LogisticRegression*), lo que permitió automatizar el proceso de entrenamiento y validación.\n",
    "\n",
    "3. **Definición de hiperparámetros**  \n",
    "   A través del *ParamGridBuilder*, se definieron combinaciones de los parámetros elasticNetParam, regParam y la fórmula de modelado para evaluar diferentes grados de regularización y relaciones entre variables.\n",
    "\n",
    "4. **Entrenamiento y validación**  \n",
    "   Se utilizó *TrainValidationSplit* con una proporción de 75/25 para entrenar y evaluar los modelos, aplicando métricas de clasificación como el AUC para determinar la calidad predictiva.\n",
    "\n",
    "5. **Evaluación de resultados**  \n",
    "   Los resultados fueron analizados y comparados a través de métricas numéricas (AUC, Accuracy, F1) y visualizaciones que permitieron identificar los modelos con mejor rendimiento general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f569eee9-0a97-40d0-a547-d58c55579e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Este conjunto de datos consta de una etiqueta categórica con dos valores (buenos o malos), una variable categórica (color) y dos variables numéricas.\n",
    "\n",
    "Si bien los datos son sintéticos, imaginemos que este conjunto de datos representa la salud del cliente de una empresa. La columna \"color\" representa una calificación de salud categórica hecha por un representante de servicio al cliente. La columna \"laboratorio\" representa la verdadera salud del cliente. Los otros dos valores son algunas medidas numéricas de actividad dentro de una aplicación (por ejemplo, minutos de permanencia en el sitio y compras).\n",
    "\n",
    "Supongamos que queremos entrenar un modelo de clasificación en el que esperamos predecir una variable binaria, la etiqueta, a partir de los otros valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ebb66e-77ff-4845-846c-0f9a0e92d8de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n|color| lab|value1|            value2|\n+-----+----+------+------------------+\n|green|good|     1|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|green|good|    12|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n|  red| bad|     2|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|green|good|     1|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|green|good|    12|14.386294994851129|\n|green|good|    12|14.386294994851129|\n|green|good|     1|14.386294994851129|\n|  red| bad|     2|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"/databricks-datasets/definitive-guide/data/simple-ml\")\n",
    "df.orderBy(\"value2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3543abe2-63f6-4415-befe-26f87eda0eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93321a5-021b-42a5-9dd3-7fd449e685b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El conjunto de datos actual no cumple con el requisito de estar en formato de Vector y, por lo tanto, debemos transformarlo al formato adecuado.\n",
    "\n",
    "Para lograr esto en nuestro ejemplo, vamos a especificar una RFormula. Este es un lenguaje declarativo para especificar transformaciones de aprendizaje automático y es fácil de usar una vez que comprende la sintaxis.\n",
    "\n",
    "Los operadores básicos de RFormula son:\n",
    "<p>\n",
    "<p>\"~\" Destino y términos separados</p>\n",
    "<p>\"+\" Términos de Concat; \"+ 0\" significa eliminar la intersección (esto significa que la intersección y de la línea que ajustaremos será 0)</p>\n",
    "<p>\"-\" Eliminar un término; \"- 1\" significa eliminar la intersección (esto significa que la intersección y de la línea que vamos a ajustar será 0; sí, esto hace lo mismo que \"+ 0\"</p>\n",
    "<p>\":\" Interacción (multiplicación de valores numéricos o valores categóricos binarizados)</p>\n",
    "<p>\".\" Todas las columnas excepto la variable objetivo / dependiente</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7dcf6f2-ac59-45e7-857f-cb10235768ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para especificar transformaciones con esta sintaxis, necesitamos importar la clase RFormula. Luego pasamos por el proceso de definir nuestra fórmula. En este caso, queremos usar todas las variables disponibles (el \".\") Y también agregar las interacciones entre valor1 y color y valor2 y color, tratándolas como características nuevas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48553f9e-a052-4943-83d1-de3472e339d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b3a458-4c51-49c0-b9fe-895bdb96ba3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El siguiente paso es ajustar el transformador RFormula a los datos para que descubra los posibles valores de cada columna.\n",
    "\n",
    "No todos los transformadores tienen este requisito, pero debido a que RFormula manejará automáticamente las variables categóricas por nosotros, necesita determinar qué columnas son categóricas y cuáles no, así como cuáles son los valores distintos de las columnas categóricas.\n",
    "\n",
    "Por esta razón, tenemos que llamar al método fit. Una vez que llamamos a fit, devuelve una versión \"entrenada\" de nuestro transformador que luego podemos usar para transformar nuestros datos.\n",
    "\n",
    "Luego llamamos a transform en ese objeto para transformar nuestros datos de entrada en los datos de salida esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b00b200-2f8c-4dc5-90a8-0f86b9757276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n|color| lab|value1|            value2|            features|label|\n+-----+----+------+------------------+--------------------+-----+\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n+-----+----+------+------------------+--------------------+-----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "fittedRF = supervised.fit(df) # Ajusta\n",
    "preparedDF = fittedRF.transform(df) # Transforma\n",
    "preparedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f165883f-0206-42d6-a8b4-263238dff374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "En la salida podemos ver el resultado de nuestra transformación: una columna llamada características que tiene nuestros datos sin procesar.\n",
    "\n",
    "Lo que sucede detrás de escena es bastante simple: RFormula inspecciona nuestros datos durante la llamada de ajuste y genera un objeto que transformará nuestros datos de acuerdo con la fórmula especificada.\n",
    "\n",
    "Cuando usamos este transformador, Spark convierte automáticamente nuestra variable categórica en Dobles para que podamos ingresarla en un modelo de aprendizaje automático.\n",
    "\n",
    "En particular, asigna un valor numérico a cada color posible.\n",
    "categoría, crea características adicionales para las variables de interacción entre colores y valor1 / valor2, y las coloca todas en un solo vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0247b43b-5569-4be0-be8f-42d40262f097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creemos ahora un conjunto de prueba simple basado en una división aleatoria de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9091baa7-5e03-4249-8a20-dbc5c18828f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff1e0c2-573e-4778-8a46-c87e93f9eeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a9d49e-9654-4ab1-b280-df511805fede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En este caso usaremos un algoritmo de clasificación llamado regresión logística.\n",
    "\n",
    "Para crear nuestro clasificador, creamos una instancia de LogisticRegression, usando la configuración predeterminada o los hiperparámetros.\n",
    "\n",
    "Luego configuramos las columnas de etiquetas y las columnas de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182d956f-f4bd-4413-a7fe-08bdbea5df8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048bff5d-05c7-4f59-8e9a-cf1eb8c9f38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Antes de comenzar a entrenar este modelo, inspeccionemos los parámetros.\n",
    "\n",
    "Este método muestra una explicación de todos los parámetros para la implementación de Spark de la regresión logística.\n",
    "\n",
    "El método \"explainParams\" existe en todos los algoritmos disponibles en MLlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9516fcd2-a68e-482f-9ff3-7e65952dffd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\nmaxIter: max number of iterations (>= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3a2ec7-503d-4200-ae97-a9523fd1ad63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Al crear una instancia de un algoritmo no entrenado, llega el momento de ajustarlo a los datos (entrenarlo). En este caso, esto devuelve un LogisticRegressionModel.\n",
    "\n",
    "Este código iniciará un trabajo de Spark para entrenar el modelo. A diferencia de las transformaciones, el ajuste de un modelo de aprendizaje automático es ansioso y se realiza de inmediato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09844fe2-9c63-49ed-9d86-fd087c7a3efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d1849c-5740-4b08-a401-637c207d7a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Una vez completado, puede usar el modelo para hacer predicciones. Lógicamente, esto significa transformar características en etiquetas.\n",
    "\n",
    "Hacemos predicciones con el método transform. Por ejemplo, podemos transformar nuestro conjunto de datos de entrenamiento para ver qué etiquetas asignó nuestro modelo a los datos de entrenamiento y cómo se comparan con los resultados reales.\n",
    "\n",
    "Realicemos esa predicción con el siguiente fragmento de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e3f3a7-6afb-4aaf-b729-9f9edb203200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n+-----+----------+\nonly showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10f9b1f-a3be-4977-9323-1f114e9dbef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nuestro siguiente paso sería evaluar manualmente este modelo y calcular métricas de rendimiento como la tasa de verdaderos positivos, la tasa de falsos negativos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b03570f-d02b-4adb-b294-16ffd6ad638e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee3fba4-fbb0-4f67-82ad-3825ec66534c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Como probablemente haya notado, si está realizando muchas transformaciones, escribir todos los pasos y realizar un seguimiento de DataFrames termina siendo bastante tedioso.\n",
    "\n",
    "Por eso Spark incluye el concepto Pipeline.\n",
    "\n",
    "Tenga en cuenta que es esencial que las instancias de transformadores o modelos no se reutilicen en diferentes Pipeline. Cree siempre una nueva instancia de un modelo antes de crear otra Pipeline.\n",
    "\n",
    "Para asegurarnos de no sobreajustarnos, crearemos un conjunto de pruebas de holdout(un método de validación) y ajustaremos nuestros hiperparámetros en función de un conjunto de validación (tenga en cuenta que creamos este conjunto de validación basado en el conjunto de datos original, no en el preparedDF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642bbd65-07cb-4bc5-9325-151810a28611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec63b9f-4c32-45ba-91a4-9486f887704c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora que tiene un conjunto de entrenamiento y prueba, creemos las stages base en nuestra Pipeline.\n",
    "\n",
    "Una stage simplemente representa un transformador o un estimador. En nuestro caso, tendremos dos estimadores. La RFomula y el LogisticRegresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b9ea54-0671-4296-a4a1-a3856ba7e4dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187e7f4c-eab8-43e5-8b6f-0b139f0398db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora, en lugar de usar manualmente nuestras transformaciones y luego ajustar nuestro modelo, simplemente las hacemos stages en la Pipeline general, como en el siguiente fragmento de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47a5eb3-92e6-4b77-821b-b27ea19ccab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48da5a5e-edfe-4086-918e-3ad0a1787906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bad831b-52b9-4717-ae1f-c8efc7aee7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora que organizó la Pipeline, el siguiente paso es el Entrenamiento.\n",
    "\n",
    "En este caso, no entrenaremos solo un modelo. Entrenaremos varias variaciones del modelo especificando diferentes combinaciones de hiperparámetros que nos gustaría que Spark probara.\n",
    "\n",
    "Luego, seleccionaremos el mejor modelo usando un evaluador que compara sus predicciones con nuestros datos de validación.\n",
    "\n",
    "Podemos probar diferentes hiperparámetros en toda la Pipeline, incluso en la fórmula de RF que usamos para manipular los datos sin procesar.\n",
    "\n",
    "En nuestro ParamGridBuilder, hay tres hiperparámetros que varían de los valores predeterminados:\n",
    "<li>Dos versiones diferentes de RFormula</li>\n",
    "<li>Tres opciones diferentes para el parámetro ElasticNet</li>\n",
    "<li>Dos opciones diferentes para el parámetro de regularización</li>\n",
    "Esto nos da un total de 12 combinaciones diferentes de estos parámetros, lo que significa que entrenaremos 12 versiones diferentes de regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a333c77-b47b-4960-ba04-367d1fd3dbe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    ".addGrid(rForm.formula, [\n",
    "\"lab ~ . + color:value1\",\n",
    "\"lab ~ . + color:value1 + color:value2\"])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    ".addGrid(lr.regParam, [0.1, 2.0])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263e8247-8310-4b89-91cf-11f8a4dce9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora que la cuadrícula está construida, es hora de especificar nuestro proceso de evaluación. El evaluador nos permite comparar de forma automática y objetiva varios modelos con la misma métrica de evaluación.\n",
    "\n",
    "En este caso usaremos el BinaryClassificationEvaluator, que tiene una serie de métricas de evaluación potenciales.\n",
    "\n",
    "En este caso usaremos areaUnderROC, que es el área total bajo la característica operativa del receptor, una medida común de desempeño de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49f51ed-47a7-40c8-b006-8789207eff4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setMetricName(\"areaUnderROC\")\\\n",
    ".setRawPredictionCol(\"prediction\")\\\n",
    ".setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575b9217-017e-4f0f-9e63-21099c91f698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora que tenemos una canalización que especifica cómo se deben transformar nuestros datos, realizaremos la selección del modelo para probar diferentes hiperparámetros en nuestro modelo de regresión logística y medir el éxito comparando su desempeño usando la métrica areaUnderROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55734280-2ae3-4094-b564-4a38687a1146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    ".setTrainRatio(0.75)\\\n",
    ".setEstimatorParamMaps(params)\\\n",
    ".setEstimator(pipeline)\\\n",
    ".setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bfbc23-af47-4812-a021-54d97a940971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejecutemos toda la Pipeline que construimos. Para revisar, la ejecución de esta canalización probará todas las versiones del modelo con el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08ff2e9-844c-44a6-93f4-1e265e2e32e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/workspace/default/ml_lab\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652be5ae-1a1c-4309-9f89-4b1b54955362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace5a8c6-21da-49e5-ae24-ea28e3ccbbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tambien se evalua cómo funciona el algoritmo con el conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc3cd41-42e4-451a-83e4-a34e5bc79af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d4828f5-5966-489c-926c-6ce6edade1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Métricas adicionales**\n",
    "A continuación se listan algunas métricas extra que se utilizarán para analizar el comportamiento de las 12 configuraciones de hiperparámetros definidas:\n",
    "\n",
    "**AUC (Área Bajo la Curva ROC)**\n",
    "- Mide la capacidad del modelo para distinguir entre clases.\n",
    "- Un valor cercano a 1 indica excelente capacidad de clasificación, mientras que 0.5 implica un modelo aleatorio.\n",
    "\n",
    "**Accuracy (Precisión global)**\n",
    "- Representa el porcentaje de predicciones correctas sobre el total de ejemplos.\n",
    "- Puede ser engañoso si las clases están desbalanceadas (por ejemplo, si una clase aparece mucho más que otra).\n",
    "\n",
    "**F1-Score (Equilibrio entre precisión y recall)**\n",
    "- Combina precisión (qué tan correctas son las predicciones positivas) y exhaustividad (cuántos positivos reales fueron detectados).\n",
    "- Es útil cuando hay clases desbalanceadas, ya que castiga tanto los falsos positivos como los falsos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592cf588-718d-4c36-b167-e838a71e1cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>formula</th><th>elasticNetParam</th><th>regParam</th><th>AUC</th><th>Accuracy</th><th>F1</th></tr></thead><tbody><tr><td>lab ~ . + color:value1 + color:value2</td><td>0.0</td><td>0.1</td><td>1.0</td><td>0.9487179487179487</td><td>0.9486504723346829</td></tr><tr><td>lab ~ . + color:value1 + color:value2</td><td>1.0</td><td>0.1</td><td>0.981578947368421</td><td>0.9487179487179487</td><td>0.9486504723346829</td></tr><tr><td>lab ~ . + color:value1</td><td>1.0</td><td>0.1</td><td>0.968421052631579</td><td>0.9487179487179487</td><td>0.9486504723346829</td></tr><tr><td>lab ~ . + color:value1</td><td>0.0</td><td>0.1</td><td>0.9631578947368421</td><td>0.9487179487179487</td><td>0.9486504723346829</td></tr><tr><td>lab ~ . + color:value1</td><td>0.0</td><td>2.0</td><td>0.9631578947368421</td><td>0.6153846153846154</td><td>0.542414107631499</td></tr><tr><td>lab ~ . + color:value1</td><td>0.5</td><td>0.1</td><td>0.9631578947368421</td><td>0.9487179487179487</td><td>0.9486504723346829</td></tr><tr><td>lab ~ . + color:value1 + color:value2</td><td>0.5</td><td>0.1</td><td>0.9631578947368421</td><td>0.8461538461538461</td><td>0.8455433455433456</td></tr><tr><td>lab ~ . + color:value1 + color:value2</td><td>0.0</td><td>2.0</td><td>0.9473684210526315</td><td>0.7435897435897436</td><td>0.7234432234432235</td></tr><tr><td>lab ~ . + color:value1</td><td>0.5</td><td>2.0</td><td>0.5</td><td>0.5128205128205128</td><td>0.3476749239461104</td></tr><tr><td>lab ~ . + color:value1</td><td>1.0</td><td>2.0</td><td>0.5</td><td>0.5128205128205128</td><td>0.3476749239461104</td></tr><tr><td>lab ~ . + color:value1 + color:value2</td><td>0.5</td><td>2.0</td><td>0.5</td><td>0.5128205128205128</td><td>0.3476749239461104</td></tr><tr><td>lab ~ . + color:value1 + color:value2</td><td>1.0</td><td>2.0</td><td>0.5</td><td>0.5128205128205128</td><td>0.3476749239461104</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "lab ~ . + color:value1 + color:value2",
         0.0,
         0.1,
         1.0,
         0.9487179487179487,
         0.9486504723346829
        ],
        [
         "lab ~ . + color:value1 + color:value2",
         1.0,
         0.1,
         0.981578947368421,
         0.9487179487179487,
         0.9486504723346829
        ],
        [
         "lab ~ . + color:value1",
         1.0,
         0.1,
         0.968421052631579,
         0.9487179487179487,
         0.9486504723346829
        ],
        [
         "lab ~ . + color:value1",
         0.0,
         0.1,
         0.9631578947368421,
         0.9487179487179487,
         0.9486504723346829
        ],
        [
         "lab ~ . + color:value1",
         0.0,
         2.0,
         0.9631578947368421,
         0.6153846153846154,
         0.542414107631499
        ],
        [
         "lab ~ . + color:value1",
         0.5,
         0.1,
         0.9631578947368421,
         0.9487179487179487,
         0.9486504723346829
        ],
        [
         "lab ~ . + color:value1 + color:value2",
         0.5,
         0.1,
         0.9631578947368421,
         0.8461538461538461,
         0.8455433455433456
        ],
        [
         "lab ~ . + color:value1 + color:value2",
         0.0,
         2.0,
         0.9473684210526315,
         0.7435897435897436,
         0.7234432234432235
        ],
        [
         "lab ~ . + color:value1",
         0.5,
         2.0,
         0.5,
         0.5128205128205128,
         0.3476749239461104
        ],
        [
         "lab ~ . + color:value1",
         1.0,
         2.0,
         0.5,
         0.5128205128205128,
         0.3476749239461104
        ],
        [
         "lab ~ . + color:value1 + color:value2",
         0.5,
         2.0,
         0.5,
         0.5128205128205128,
         0.3476749239461104
        ],
        [
         "lab ~ . + color:value1 + color:value2",
         1.0,
         2.0,
         0.5,
         0.5128205128205128,
         0.3476749239461104
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "formula",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "elasticNetParam",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "regParam",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "AUC",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Accuracy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "F1",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Evaluadores\n",
    "binary_eval = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"label\")\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluar todas las combinaciones de hiperparámetros con métricas (AUC, Accuracy, F1)\n",
    "for paramMap in params:\n",
    "    model = pipeline.fit(train, paramMap)\n",
    "    predictions = model.transform(test)\n",
    "    \n",
    "    auc = binary_eval.evaluate(predictions)\n",
    "    acc = multi_eval.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "    f1 = multi_eval.setMetricName(\"f1\").evaluate(predictions)\n",
    "    \n",
    "    result = {p.name: v for p, v in paramMap.items()}\n",
    "    result.update({\"AUC\": auc, \"Accuracy\": acc, \"F1\": f1})\n",
    "    results.append(result)\n",
    "\n",
    "# Convertir a DataFrame para análisis\n",
    "metrics_df = pd.DataFrame(results)\n",
    "display(metrics_df.sort_values(by=\"AUC\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8f6d03-b08b-44f0-af5e-13fb637f3729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análisis de métricas \n",
    "\n",
    "**1. Mejores combinaciones**\n",
    "\n",
    "Las configuraciones con elasticNetParam = 0 y regParam = 0.1 (líneas 1, 4, 5) obtienen los mejores valores globales, con:\n",
    "\n",
    "AUC ≈ 1.0, Accuracy ≈ 0.95, F1 ≈ 0.95\n",
    "\n",
    "Esto indica que el modelo clasifica casi perfectamente, mostrando una excelente capacidad predictiva y estabilidad.\n",
    "\n",
    "**2. Combinaciones con elasticNetParam alto (0.5 o 1.0)**\n",
    "\n",
    "A medida que aumenta la regularización (regParam = 2 o elasticNetParam alto), los valores de AUC, Accuracy y F1 tienden a disminuir.\n",
    "Esto sugiere que una regularización excesiva deteriora el rendimiento, probablemente por subajuste (underfitting).\n",
    "\n",
    "**3. Comparación entre fórmulas (RFormula)**\n",
    "\n",
    "Las combinaciones que incluyen color:value2 tienden a obtener ligeramente mejores métricas, lo que indica que esta interacción aporta información relevante para la clasificación.\n",
    "Sin embargo, la diferencia no es muy grande, por lo que ambos enfoques (color:value1 vs color:value1 + color:value2) son válidos.\n",
    "\n",
    "**4. Tendencias generales**\n",
    "\n",
    "El modelo parece muy sensible a los parámetros de regularización.\n",
    "Con valores bajos de regParam y elasticNetParam, la red logra un desempeño casi perfecto, pero con valores altos pierde capacidad de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cec007-700d-4ee1-a818-bea559f2b24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9500000000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03452886-e000-43d0-a7d7-7bcc9369cb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b683b0-bced-46ef-892c-22068be6bb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Los resultados evidenciaron que las combinaciones con **baja regularización** (regParam = 0.1, elasticNetParam = 0) alcanzaron el mejor desempeño, obteniendo **AUC, Accuracy y F1 cercanos a 1**, lo que indica una excelente capacidad de clasificación del modelo.\n",
    "\n",
    "2. Se observó que un **aumento en los valores de regularización** provoca una disminución significativa en el rendimiento, lo cual sugiere la presencia de **subajuste** cuando el modelo es penalizado en exceso.\n",
    "\n",
    "3. La inclusión de la variable de interacción **color:value2** aportó una ligera mejora en las métricas, evidenciando que la consideración de relaciones no lineales puede aumentar la capacidad predictiva del modelo.\n",
    "\n",
    "4. Las métricas AUC, Accuracy y F1 demostraron ser complementarias para evaluar el rendimiento del modelo: mientras AUC refleja la capacidad discriminativa, Accuracy y F1 permiten medir la proporción de aciertos y el balance entre precisión y exhaustividad.\n",
    "\n",
    "5. En general, el análisis permitió comprender cómo la **tuning de hiperparámetros** dentro de un entorno distribuido como PySpark contribuye a optimizar los modelos predictivos, mejorando su precisión y estabilidad en tareas de clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63297a1f-b14c-49f6-8565-65184bdc0e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_Laboratorio_González",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}